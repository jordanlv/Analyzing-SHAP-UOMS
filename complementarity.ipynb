{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b8ad00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c4dc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    compute_shap_similarity_pearson,\n",
    "    compute_ndcg_similarity,\n",
    "    compute_pred_jaccard,\n",
    "    compute_aucpr,\n",
    "    compute_score_correlations,\n",
    "    load_nested_results,\n",
    ")\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4190b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = load_nested_results(\"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a605a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = list(all_results.keys())\n",
    "models_names = sorted(list(all_results[dataset_names[0]].keys() - {\"ground_truth\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21a0d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import average_precision_score\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "\n",
    "def aggreg_rank(y_true, y_preds):\n",
    "    ranks = [rankdata(scores, \"average\") for scores in y_preds]\n",
    "    mean_ranks = np.mean(ranks, axis=0)\n",
    "    return average_precision_score(y_true, mean_ranks) \n",
    "\n",
    "\n",
    "scores = defaultdict(lambda: defaultdict(list))\n",
    "n_models_ensemblist = 3\n",
    "\n",
    "for dataset in dataset_names:\n",
    "    print(dataset)\n",
    "\n",
    "    y_true_folds = all_results[dataset][\"ground_truth\"]\n",
    "\n",
    "    # Compute similarities on the full set of models first\n",
    "    dist_shap_sim = 1 - compute_shap_similarity_pearson(all_results[dataset])[0]\n",
    "    dist_ndcg_sim = 1 - compute_ndcg_similarity(all_results[dataset])[0]\n",
    "    dist_scores_sim = 1 - compute_score_correlations(all_results[dataset])[0]\n",
    "    dist_jaccard_sim = 1 - compute_pred_jaccard(all_results[dataset])[0]\n",
    "\n",
    "    for comb in combinations(range(len(models_names)), n_models_ensemblist):\n",
    "        metric = []\n",
    "\n",
    "        for fold in range(len(y_true_folds)):\n",
    "            y_true = y_true_folds[fold]\n",
    "\n",
    "            metric.append(\n",
    "                aggreg_rank(\n",
    "                    y_true,\n",
    "                    np.array(\n",
    "                        [\n",
    "                            all_results[dataset][models_names[i]][fold][\"scores\"]\n",
    "                            for i in comb\n",
    "                        ]\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        dists_shap = []\n",
    "        dists_ndcg = []\n",
    "        dists_scores = []\n",
    "        dists_jaccard = []\n",
    "\n",
    "        # Metric aggregation\n",
    "        for i in range(n_models_ensemblist):\n",
    "            for j in range(i + 1, n_models_ensemblist):\n",
    "                dists_shap.append(dist_shap_sim[comb[i], comb[j]])\n",
    "                dists_ndcg.append(dist_ndcg_sim[comb[i], comb[j]])\n",
    "                dists_scores.append(dist_scores_sim[comb[i], comb[j]])\n",
    "                dists_jaccard.append(dist_jaccard_sim[comb[i], comb[j]])\n",
    "\n",
    "        scores[dataset][\"name\"].append(f\"{'-'.join([models_names[i] for i in comb])}\")\n",
    "        scores[dataset][\"mcc\"].append(np.nanmean(metric))\n",
    "        scores[dataset][\"ndcg\"].append(np.mean(dists_ndcg))\n",
    "        scores[dataset][\"shap\"].append(np.mean(dists_shap))\n",
    "        scores[dataset][\"scores\"].append(np.mean(dists_scores))\n",
    "        scores[dataset][\"jaccard\"].append(np.mean(dists_jaccard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9a2f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "data = []\n",
    "\n",
    "for dataset in dataset_names:\n",
    "    mcc = scores[dataset][\"mcc\"]\n",
    "    r, _ = pearsonr(mcc, scores[dataset][\"shap\"])\n",
    "    r_w, _ = pearsonr(mcc, scores[dataset][\"ndcg\"])\n",
    "    r_s, _ = pearsonr(mcc, scores[dataset][\"scores\"])\n",
    "    r_J, _ = pearsonr(mcc, scores[dataset][\"jaccard\"])\n",
    "\n",
    "    data.append(\n",
    "        {\"Dataset\": dataset, \"shap\": r, \"NDCG\": r_w, \"Scores\": r_s, \"Jaccard\": r_J}\n",
    "    )\n",
    "\n",
    "df = pd.DataFrame(data).set_index(\"Dataset\")\n",
    "styled_df = df.style.highlight_max(axis=1, color=\"red\").format(precision=3)\n",
    "\n",
    "styled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faf173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"shap\"] *= 100\n",
    "df[\"NDCG\"] *= 100\n",
    "df[\"Scores\"] *= 100\n",
    "df[\"Jaccard\"] *= 100\n",
    "df.round(1)\n",
    "print((df).T.to_latex(float_format=\"%.0f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff61c891",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((df.mean(axis=0)).T.to_latex(float_format=\"%.0f\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shapspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
